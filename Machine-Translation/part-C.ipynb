{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# copied over from last year DL course, adapt to this year's assgn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset wmt16/de-en to C:/Users/Gautam/.cache/huggingface/datasets/wmt16/de-en/1.0.0/746749a11d25c02058042da7502d973ff410e73457f3d305fc1177dc0e8c4227...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d0cfbcd3f6040a697837e653375755d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6956db8e94a44fcfb49fda839d138ffc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "BadZipFile",
     "evalue": "File is not a zip file",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mBadZipFile\u001b[0m                                Traceback (most recent call last)\n",
      "\u001b[1;32mc:\\Users\\Gautam\\Desktop\\Assignments\\DL\\A3\\A3.ipynb Cell 34\u001b[0m in \u001b[0;36m4\n",
      "\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Gautam/Desktop/Assignments/DL/A3/A3.ipynb#X43sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mipywidgets\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mwidgets\u001b[39;00m\n",
      "\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Gautam/Desktop/Assignments/DL/A3/A3.ipynb#X43sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdatasets\u001b[39;00m \u001b[39mimport\u001b[39;00m load_dataset\n",
      "\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Gautam/Desktop/Assignments/DL/A3/A3.ipynb#X43sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m dataset \u001b[39m=\u001b[39m load_dataset(\u001b[39m\"\u001b[39;49m\u001b[39mwmt16\u001b[39;49m\u001b[39m\"\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39mde-en\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Gautam/Desktop/Assignments/DL/A3/A3.ipynb#X43sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m training_words \u001b[39m=\u001b[39m dataset[\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m]\n",
      "\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Gautam/Desktop/Assignments/DL/A3/A3.ipynb#X43sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mtype\u001b[39m(training_words))\n",
      "\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\datasets\\load.py:1791\u001b[0m, in \u001b[0;36mload_dataset\u001b[1;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, num_proc, storage_options, **config_kwargs)\u001b[0m\n",
      "\u001b[0;32m   1788\u001b[0m try_from_hf_gcs \u001b[39m=\u001b[39m path \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m _PACKAGED_DATASETS_MODULES\n",
      "\u001b[0;32m   1790\u001b[0m \u001b[39m# Download and prepare data\u001b[39;00m\n",
      "\u001b[1;32m-> 1791\u001b[0m builder_instance\u001b[39m.\u001b[39;49mdownload_and_prepare(\n",
      "\u001b[0;32m   1792\u001b[0m     download_config\u001b[39m=\u001b[39;49mdownload_config,\n",
      "\u001b[0;32m   1793\u001b[0m     download_mode\u001b[39m=\u001b[39;49mdownload_mode,\n",
      "\u001b[0;32m   1794\u001b[0m     verification_mode\u001b[39m=\u001b[39;49mverification_mode,\n",
      "\u001b[0;32m   1795\u001b[0m     try_from_hf_gcs\u001b[39m=\u001b[39;49mtry_from_hf_gcs,\n",
      "\u001b[0;32m   1796\u001b[0m     num_proc\u001b[39m=\u001b[39;49mnum_proc,\n",
      "\u001b[0;32m   1797\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n",
      "\u001b[0;32m   1798\u001b[0m )\n",
      "\u001b[0;32m   1800\u001b[0m \u001b[39m# Build dataset for splits\u001b[39;00m\n",
      "\u001b[0;32m   1801\u001b[0m keep_in_memory \u001b[39m=\u001b[39m (\n",
      "\u001b[0;32m   1802\u001b[0m     keep_in_memory \u001b[39mif\u001b[39;00m keep_in_memory \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m is_small_dataset(builder_instance\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mdataset_size)\n",
      "\u001b[0;32m   1803\u001b[0m )\n",
      "\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\datasets\\builder.py:891\u001b[0m, in \u001b[0;36mDatasetBuilder.download_and_prepare\u001b[1;34m(self, output_dir, download_config, download_mode, verification_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\u001b[0m\n",
      "\u001b[0;32m    889\u001b[0m     \u001b[39mif\u001b[39;00m num_proc \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;32m    890\u001b[0m         prepare_split_kwargs[\u001b[39m\"\u001b[39m\u001b[39mnum_proc\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m num_proc\n",
      "\u001b[1;32m--> 891\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_download_and_prepare(\n",
      "\u001b[0;32m    892\u001b[0m         dl_manager\u001b[39m=\u001b[39mdl_manager,\n",
      "\u001b[0;32m    893\u001b[0m         verification_mode\u001b[39m=\u001b[39mverification_mode,\n",
      "\u001b[0;32m    894\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mprepare_split_kwargs,\n",
      "\u001b[0;32m    895\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mdownload_and_prepare_kwargs,\n",
      "\u001b[0;32m    896\u001b[0m     )\n",
      "\u001b[0;32m    897\u001b[0m \u001b[39m# Sync info\u001b[39;00m\n",
      "\u001b[0;32m    898\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mdataset_size \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(split\u001b[39m.\u001b[39mnum_bytes \u001b[39mfor\u001b[39;00m split \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39msplits\u001b[39m.\u001b[39mvalues())\n",
      "\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\datasets\\builder.py:1651\u001b[0m, in \u001b[0;36mGeneratorBasedBuilder._download_and_prepare\u001b[1;34m(self, dl_manager, verification_mode, **prepare_splits_kwargs)\u001b[0m\n",
      "\u001b[0;32m   1650\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_download_and_prepare\u001b[39m(\u001b[39mself\u001b[39m, dl_manager, verification_mode, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mprepare_splits_kwargs):\n",
      "\u001b[1;32m-> 1651\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m_download_and_prepare(\n",
      "\u001b[0;32m   1652\u001b[0m         dl_manager,\n",
      "\u001b[0;32m   1653\u001b[0m         verification_mode,\n",
      "\u001b[0;32m   1654\u001b[0m         check_duplicate_keys\u001b[39m=\u001b[39mverification_mode \u001b[39m==\u001b[39m VerificationMode\u001b[39m.\u001b[39mBASIC_CHECKS\n",
      "\u001b[0;32m   1655\u001b[0m         \u001b[39mor\u001b[39;00m verification_mode \u001b[39m==\u001b[39m VerificationMode\u001b[39m.\u001b[39mALL_CHECKS,\n",
      "\u001b[0;32m   1656\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mprepare_splits_kwargs,\n",
      "\u001b[0;32m   1657\u001b[0m     )\n",
      "\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\datasets\\builder.py:964\u001b[0m, in \u001b[0;36mDatasetBuilder._download_and_prepare\u001b[1;34m(self, dl_manager, verification_mode, **prepare_split_kwargs)\u001b[0m\n",
      "\u001b[0;32m    962\u001b[0m split_dict \u001b[39m=\u001b[39m SplitDict(dataset_name\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname)\n",
      "\u001b[0;32m    963\u001b[0m split_generators_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_split_generators_kwargs(prepare_split_kwargs)\n",
      "\u001b[1;32m--> 964\u001b[0m split_generators \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_split_generators(dl_manager, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39msplit_generators_kwargs)\n",
      "\u001b[0;32m    966\u001b[0m \u001b[39m# Checksums verification\u001b[39;00m\n",
      "\u001b[0;32m    967\u001b[0m \u001b[39mif\u001b[39;00m verification_mode \u001b[39m==\u001b[39m VerificationMode\u001b[39m.\u001b[39mALL_CHECKS \u001b[39mand\u001b[39;00m dl_manager\u001b[39m.\u001b[39mrecord_checksums:\n",
      "\n",
      "File \u001b[1;32m~\\.cache\\huggingface\\modules\\datasets_modules\\datasets\\wmt16\\746749a11d25c02058042da7502d973ff410e73457f3d305fc1177dc0e8c4227\\wmt_utils.py:754\u001b[0m, in \u001b[0;36mWmt._split_generators\u001b[1;34m(self, dl_manager)\u001b[0m\n",
      "\u001b[0;32m    751\u001b[0m         urls_to_download[ss_name] \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39mget_url(source)\n",
      "\u001b[0;32m    753\u001b[0m \u001b[39m# Download and extract files from URLs.\u001b[39;00m\n",
      "\u001b[1;32m--> 754\u001b[0m downloaded_files \u001b[39m=\u001b[39m dl_manager\u001b[39m.\u001b[39;49mdownload_and_extract(urls_to_download)\n",
      "\u001b[0;32m    755\u001b[0m \u001b[39m# Extract manually downloaded files.\u001b[39;00m\n",
      "\u001b[0;32m    756\u001b[0m manual_files \u001b[39m=\u001b[39m dl_manager\u001b[39m.\u001b[39mextract(manual_paths_dict)\n",
      "\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\datasets\\download\\download_manager.py:564\u001b[0m, in \u001b[0;36mDownloadManager.download_and_extract\u001b[1;34m(self, url_or_urls)\u001b[0m\n",
      "\u001b[0;32m    548\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdownload_and_extract\u001b[39m(\u001b[39mself\u001b[39m, url_or_urls):\n",
      "\u001b[0;32m    549\u001b[0m     \u001b[39m\"\"\"Download and extract given `url_or_urls`.\u001b[39;00m\n",
      "\u001b[0;32m    550\u001b[0m \n",
      "\u001b[0;32m    551\u001b[0m \u001b[39m    Is roughly equivalent to:\u001b[39;00m\n",
      "\u001b[1;32m   (...)\u001b[0m\n",
      "\u001b[0;32m    562\u001b[0m \u001b[39m        extracted_path(s): `str`, extracted paths of given URL(s).\u001b[39;00m\n",
      "\u001b[0;32m    563\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n",
      "\u001b[1;32m--> 564\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mextract(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdownload(url_or_urls))\n",
      "\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\datasets\\download\\download_manager.py:536\u001b[0m, in \u001b[0;36mDownloadManager.extract\u001b[1;34m(self, path_or_paths, num_proc)\u001b[0m\n",
      "\u001b[0;32m    534\u001b[0m \u001b[39mif\u001b[39;00m download_config\u001b[39m.\u001b[39mdownload_desc \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;32m    535\u001b[0m     download_config\u001b[39m.\u001b[39mdownload_desc \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mDownloading data\u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[1;32m--> 536\u001b[0m extracted_paths \u001b[39m=\u001b[39m map_nested(\n",
      "\u001b[0;32m    537\u001b[0m     partial(cached_path, download_config\u001b[39m=\u001b[39;49mdownload_config),\n",
      "\u001b[0;32m    538\u001b[0m     path_or_paths,\n",
      "\u001b[0;32m    539\u001b[0m     num_proc\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49mnum_proc,\n",
      "\u001b[0;32m    540\u001b[0m     disable_tqdm\u001b[39m=\u001b[39;49m\u001b[39mnot\u001b[39;49;00m is_progress_bar_enabled(),\n",
      "\u001b[0;32m    541\u001b[0m     desc\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mExtracting data files\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n",
      "\u001b[0;32m    542\u001b[0m )\n",
      "\u001b[0;32m    543\u001b[0m path_or_paths \u001b[39m=\u001b[39m NestedDataStructure(path_or_paths)\n",
      "\u001b[0;32m    544\u001b[0m extracted_paths \u001b[39m=\u001b[39m NestedDataStructure(extracted_paths)\n",
      "\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\datasets\\utils\\py_utils.py:443\u001b[0m, in \u001b[0;36mmap_nested\u001b[1;34m(function, data_struct, dict_only, map_list, map_tuple, map_numpy, num_proc, parallel_min_length, types, disable_tqdm, desc)\u001b[0m\n",
      "\u001b[0;32m    441\u001b[0m     num_proc \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "\u001b[0;32m    442\u001b[0m \u001b[39mif\u001b[39;00m num_proc \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \u001b[39mor\u001b[39;00m \u001b[39mlen\u001b[39m(iterable) \u001b[39m<\u001b[39m parallel_min_length:\n",
      "\u001b[1;32m--> 443\u001b[0m     mapped \u001b[39m=\u001b[39m [\n",
      "\u001b[0;32m    444\u001b[0m         _single_map_nested((function, obj, types, \u001b[39mNone\u001b[39;00m, \u001b[39mTrue\u001b[39;00m, \u001b[39mNone\u001b[39;00m))\n",
      "\u001b[0;32m    445\u001b[0m         \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m logging\u001b[39m.\u001b[39mtqdm(iterable, disable\u001b[39m=\u001b[39mdisable_tqdm, desc\u001b[39m=\u001b[39mdesc)\n",
      "\u001b[0;32m    446\u001b[0m     ]\n",
      "\u001b[0;32m    447\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;32m    448\u001b[0m     num_proc \u001b[39m=\u001b[39m num_proc \u001b[39mif\u001b[39;00m num_proc \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(iterable) \u001b[39melse\u001b[39;00m \u001b[39mlen\u001b[39m(iterable)\n",
      "\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\datasets\\utils\\py_utils.py:444\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n",
      "\u001b[0;32m    441\u001b[0m     num_proc \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "\u001b[0;32m    442\u001b[0m \u001b[39mif\u001b[39;00m num_proc \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \u001b[39mor\u001b[39;00m \u001b[39mlen\u001b[39m(iterable) \u001b[39m<\u001b[39m parallel_min_length:\n",
      "\u001b[0;32m    443\u001b[0m     mapped \u001b[39m=\u001b[39m [\n",
      "\u001b[1;32m--> 444\u001b[0m         _single_map_nested((function, obj, types, \u001b[39mNone\u001b[39;49;00m, \u001b[39mTrue\u001b[39;49;00m, \u001b[39mNone\u001b[39;49;00m))\n",
      "\u001b[0;32m    445\u001b[0m         \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m logging\u001b[39m.\u001b[39mtqdm(iterable, disable\u001b[39m=\u001b[39mdisable_tqdm, desc\u001b[39m=\u001b[39mdesc)\n",
      "\u001b[0;32m    446\u001b[0m     ]\n",
      "\u001b[0;32m    447\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;32m    448\u001b[0m     num_proc \u001b[39m=\u001b[39m num_proc \u001b[39mif\u001b[39;00m num_proc \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(iterable) \u001b[39melse\u001b[39;00m \u001b[39mlen\u001b[39m(iterable)\n",
      "\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\datasets\\utils\\py_utils.py:363\u001b[0m, in \u001b[0;36m_single_map_nested\u001b[1;34m(args)\u001b[0m\n",
      "\u001b[0;32m    361\u001b[0m     \u001b[39mreturn\u001b[39;00m {k: _single_map_nested((function, v, types, \u001b[39mNone\u001b[39;00m, \u001b[39mTrue\u001b[39;00m, \u001b[39mNone\u001b[39;00m)) \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m pbar}\n",
      "\u001b[0;32m    362\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[1;32m--> 363\u001b[0m     mapped \u001b[39m=\u001b[39m [_single_map_nested((function, v, types, \u001b[39mNone\u001b[39;00m, \u001b[39mTrue\u001b[39;00m, \u001b[39mNone\u001b[39;00m)) \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m pbar]\n",
      "\u001b[0;32m    364\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_struct, \u001b[39mlist\u001b[39m):\n",
      "\u001b[0;32m    365\u001b[0m         \u001b[39mreturn\u001b[39;00m mapped\n",
      "\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\datasets\\utils\\py_utils.py:363\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n",
      "\u001b[0;32m    361\u001b[0m     \u001b[39mreturn\u001b[39;00m {k: _single_map_nested((function, v, types, \u001b[39mNone\u001b[39;00m, \u001b[39mTrue\u001b[39;00m, \u001b[39mNone\u001b[39;00m)) \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m pbar}\n",
      "\u001b[0;32m    362\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[1;32m--> 363\u001b[0m     mapped \u001b[39m=\u001b[39m [_single_map_nested((function, v, types, \u001b[39mNone\u001b[39;49;00m, \u001b[39mTrue\u001b[39;49;00m, \u001b[39mNone\u001b[39;49;00m)) \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m pbar]\n",
      "\u001b[0;32m    364\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_struct, \u001b[39mlist\u001b[39m):\n",
      "\u001b[0;32m    365\u001b[0m         \u001b[39mreturn\u001b[39;00m mapped\n",
      "\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\datasets\\utils\\py_utils.py:346\u001b[0m, in \u001b[0;36m_single_map_nested\u001b[1;34m(args)\u001b[0m\n",
      "\u001b[0;32m    344\u001b[0m \u001b[39m# Singleton first to spare some computation\u001b[39;00m\n",
      "\u001b[0;32m    345\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(data_struct, \u001b[39mdict\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(data_struct, types):\n",
      "\u001b[1;32m--> 346\u001b[0m     \u001b[39mreturn\u001b[39;00m function(data_struct)\n",
      "\u001b[0;32m    348\u001b[0m \u001b[39m# Reduce logging to keep things readable in multiprocessing with tqdm\u001b[39;00m\n",
      "\u001b[0;32m    349\u001b[0m \u001b[39mif\u001b[39;00m rank \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m logging\u001b[39m.\u001b[39mget_verbosity() \u001b[39m<\u001b[39m logging\u001b[39m.\u001b[39mWARNING:\n",
      "\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\datasets\\utils\\file_utils.py:212\u001b[0m, in \u001b[0;36mcached_path\u001b[1;34m(url_or_filename, download_config, **download_kwargs)\u001b[0m\n",
      "\u001b[0;32m    209\u001b[0m     \u001b[39mreturn\u001b[39;00m output_path\n",
      "\u001b[0;32m    211\u001b[0m \u001b[39mif\u001b[39;00m download_config\u001b[39m.\u001b[39mextract_compressed_file:\n",
      "\u001b[1;32m--> 212\u001b[0m     output_path \u001b[39m=\u001b[39m ExtractManager(cache_dir\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49mcache_dir)\u001b[39m.\u001b[39;49mextract(\n",
      "\u001b[0;32m    213\u001b[0m         output_path, force_extract\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49mforce_extract\n",
      "\u001b[0;32m    214\u001b[0m     )\n",
      "\u001b[0;32m    216\u001b[0m \u001b[39mreturn\u001b[39;00m output_path\n",
      "\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\datasets\\utils\\extract.py:48\u001b[0m, in \u001b[0;36mExtractManager.extract\u001b[1;34m(self, input_path, force_extract)\u001b[0m\n",
      "\u001b[0;32m     46\u001b[0m output_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_output_path(input_path)\n",
      "\u001b[0;32m     47\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_do_extract(output_path, force_extract):\n",
      "\u001b[1;32m---> 48\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mextractor\u001b[39m.\u001b[39;49mextract(input_path, output_path, extractor_format)\n",
      "\u001b[0;32m     49\u001b[0m \u001b[39mreturn\u001b[39;00m output_path\n",
      "\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\datasets\\utils\\extract.py:344\u001b[0m, in \u001b[0;36mExtractor.extract\u001b[1;34m(cls, input_path, output_path, extractor_format, extractor)\u001b[0m\n",
      "\u001b[0;32m    342\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;32m    343\u001b[0m         extractor \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mextractors[extractor_format]\n",
      "\u001b[1;32m--> 344\u001b[0m     \u001b[39mreturn\u001b[39;00m extractor\u001b[39m.\u001b[39;49mextract(input_path, output_path)\n",
      "\u001b[0;32m    345\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;32m    346\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n",
      "\u001b[0;32m    347\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mParameter \u001b[39m\u001b[39m'\u001b[39m\u001b[39mextractor_format\u001b[39m\u001b[39m'\u001b[39m\u001b[39m was made required in version 2.4.0 and not passing it will raise an \u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[0;32m    348\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mexception in 3.0.0.\u001b[39m\u001b[39m\"\u001b[39m,\n",
      "\u001b[0;32m    349\u001b[0m         category\u001b[39m=\u001b[39m\u001b[39mFutureWarning\u001b[39;00m,\n",
      "\u001b[0;32m    350\u001b[0m     )\n",
      "\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\datasets\\utils\\extract.py:189\u001b[0m, in \u001b[0;36mZipExtractor.extract\u001b[1;34m(input_path, output_path)\u001b[0m\n",
      "\u001b[0;32m    186\u001b[0m \u001b[39m@staticmethod\u001b[39m\n",
      "\u001b[0;32m    187\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mextract\u001b[39m(input_path: Union[Path, \u001b[39mstr\u001b[39m], output_path: Union[Path, \u001b[39mstr\u001b[39m]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;32m    188\u001b[0m     os\u001b[39m.\u001b[39mmakedirs(output_path, exist_ok\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[1;32m--> 189\u001b[0m     \u001b[39mwith\u001b[39;00m zipfile\u001b[39m.\u001b[39;49mZipFile(input_path, \u001b[39m\"\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m\"\u001b[39;49m) \u001b[39mas\u001b[39;00m zip_file:\n",
      "\u001b[0;32m    190\u001b[0m         zip_file\u001b[39m.\u001b[39mextractall(output_path)\n",
      "\u001b[0;32m    191\u001b[0m         zip_file\u001b[39m.\u001b[39mclose()\n",
      "\n",
      "File \u001b[1;32mc:\\Python310\\lib\\zipfile.py:1267\u001b[0m, in \u001b[0;36mZipFile.__init__\u001b[1;34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps)\u001b[0m\n",
      "\u001b[0;32m   1265\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[0;32m   1266\u001b[0m     \u001b[39mif\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m:\n",
      "\u001b[1;32m-> 1267\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_RealGetContents()\n",
      "\u001b[0;32m   1268\u001b[0m     \u001b[39melif\u001b[39;00m mode \u001b[39min\u001b[39;00m (\u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mx\u001b[39m\u001b[39m'\u001b[39m):\n",
      "\u001b[0;32m   1269\u001b[0m         \u001b[39m# set the modified flag so central directory gets written\u001b[39;00m\n",
      "\u001b[0;32m   1270\u001b[0m         \u001b[39m# even if no files are added to the archive\u001b[39;00m\n",
      "\u001b[0;32m   1271\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_didModify \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\n",
      "File \u001b[1;32mc:\\Python310\\lib\\zipfile.py:1334\u001b[0m, in \u001b[0;36mZipFile._RealGetContents\u001b[1;34m(self)\u001b[0m\n",
      "\u001b[0;32m   1332\u001b[0m     \u001b[39mraise\u001b[39;00m BadZipFile(\u001b[39m\"\u001b[39m\u001b[39mFile is not a zip file\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;32m   1333\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m endrec:\n",
      "\u001b[1;32m-> 1334\u001b[0m     \u001b[39mraise\u001b[39;00m BadZipFile(\u001b[39m\"\u001b[39m\u001b[39mFile is not a zip file\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;32m   1335\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdebug \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n",
      "\u001b[0;32m   1336\u001b[0m     \u001b[39mprint\u001b[39m(endrec)\n",
      "\n",
      "\u001b[1;31mBadZipFile\u001b[0m: File is not a zip file"
     ]
    }
   ],
   "source": [
    "import tqdm as notebook_tqdm\n",
    "import ipywidgets as widgets\n",
    "from datasets import load_dataset\n",
    "dataset = load_daaset(\"wmt16\",'de-en')\n",
    "\n",
    "training_words = dataset['train']\n",
    "print(type(training_words))\n",
    "training_words=training_words.select(range(100000))\n",
    "\n",
    "eval_words = dataset['validation']\n",
    "test_words = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gautam\\AppData\\Roaming\\Python\\Python310\\site-packages\\transformers\\models\\t5\\tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-small automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_lang = \"en\"\n",
    "target_lang = \"de\"\n",
    "prefix = \"translate English to German: \"\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + example[source_lang] for example in examples[\"translation\"]]\n",
    "    targets = [example[target_lang] for example in examples[\"translation\"]]\n",
    "    model_inputs = tokenizer(inputs, text_target=targets, max_length=128, truncation=True)\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\Gautam\\.cache\\huggingface\\datasets\\wmt16\\de-en\\1.0.0\\746749a11d25c02058042da7502d973ff410e73457f3d305fc1177dc0e8c4227\\cache-0b400da7fc23c8aa.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Gautam\\.cache\\huggingface\\datasets\\wmt16\\de-en\\1.0.0\\746749a11d25c02058042da7502d973ff410e73457f3d305fc1177dc0e8c4227\\cache-ec2f9a3876d81dd4.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Gautam\\.cache\\huggingface\\datasets\\wmt16\\de-en\\1.0.0\\746749a11d25c02058042da7502d973ff410e73457f3d305fc1177dc0e8c4227\\cache-23e41396a5aab6dc.arrow\n"
     ]
    }
   ],
   "source": [
    "tokenized__train = training_words.map(preprocess_function, batched=True)\n",
    "tokenized_eval= eval_words.map(preprocess_function, batched=True)\n",
    "tokenized_test= test_words.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "blue = evaluate.load(\"bleu\")\n",
    "rouge= evaluate.load(\"rouge\")\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [[label.strip()] for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "                          \n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    result_bleu = blue.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    # result_bleu2 = bleu2.compute(predictions=decoded_preds, references=decoded_labels)[0]\n",
    "    rougeL = rouge.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "\n",
    "    result = {\"bleu1\": result_bleu['precisions'][0],\"bleu2\": result_bleu['precisions'][1],\"rougeL\":rougeL['rougeL']}\n",
    "\n",
    "    # prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    # result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    # result = {k: round(v, 4) for k, v in result.items()}\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def compute_metrics(eval_preds):\n",
    "#     preds, labels = eval_preds\n",
    "#     if isinstance(preds, tuple):\n",
    "#         preds = preds[0]\n",
    "#     decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "                          \n",
    "#     labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "#     decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "#     decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "#     result_bleu = blue.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "#     result_rougeL = rouge.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "\n",
    "#     result = {\"bleu1\": result_bleu,'rouge_l':result_rougeL}\n",
    "\n",
    "    # prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    # result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    # result = {k: round(v, 4) for k, v in result.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 False\n",
      "1 False\n",
      "2 False\n",
      "3 False\n",
      "4 False\n",
      "5 False\n",
      "6 False\n",
      "7 False\n",
      "8 False\n",
      "9 False\n",
      "10 False\n",
      "11 False\n",
      "12 False\n",
      "13 False\n",
      "14 False\n",
      "15 False\n",
      "16 False\n",
      "17 False\n",
      "18 False\n",
      "19 False\n",
      "20 False\n",
      "21 False\n",
      "22 False\n",
      "23 False\n",
      "24 False\n",
      "25 False\n",
      "26 False\n",
      "27 False\n",
      "28 False\n",
      "29 False\n",
      "30 False\n",
      "31 False\n",
      "32 False\n",
      "33 False\n",
      "34 False\n",
      "35 True\n",
      "36 True\n",
      "37 True\n",
      "38 True\n",
      "39 True\n",
      "40 True\n",
      "41 True\n",
      "42 True\n",
      "43 True\n",
      "44 True\n",
      "45 True\n",
      "46 True\n",
      "47 True\n",
      "48 True\n",
      "49 True\n",
      "50 True\n",
      "51 True\n",
      "52 True\n",
      "53 True\n",
      "54 True\n",
      "55 True\n",
      "56 True\n",
      "57 True\n",
      "58 True\n",
      "59 True\n",
      "60 True\n",
      "61 True\n",
      "62 True\n",
      "63 True\n",
      "64 True\n",
      "65 True\n",
      "66 True\n",
      "67 True\n",
      "68 True\n",
      "69 True\n",
      "70 True\n",
      "71 True\n",
      "72 True\n",
      "73 True\n",
      "74 True\n",
      "75 True\n",
      "76 True\n",
      "77 True\n",
      "78 True\n",
      "79 True\n",
      "80 True\n",
      "81 True\n",
      "82 True\n",
      "83 True\n",
      "84 True\n",
      "85 True\n",
      "86 True\n",
      "87 True\n",
      "88 True\n",
      "89 True\n",
      "90 True\n",
      "91 True\n",
      "92 True\n",
      "93 True\n",
      "94 True\n",
      "95 True\n",
      "96 True\n",
      "97 True\n",
      "98 True\n",
      "99 True\n",
      "100 True\n",
      "101 True\n",
      "102 True\n",
      "103 True\n",
      "104 True\n",
      "105 True\n",
      "106 True\n",
      "107 True\n",
      "108 True\n",
      "109 True\n",
      "110 True\n",
      "111 True\n",
      "112 True\n",
      "113 True\n",
      "114 True\n",
      "115 True\n",
      "116 True\n",
      "117 True\n",
      "118 True\n",
      "119 True\n",
      "120 True\n",
      "121 True\n",
      "122 True\n",
      "123 True\n",
      "124 True\n",
      "125 True\n",
      "126 True\n",
      "127 True\n",
      "128 True\n",
      "129 True\n",
      "130 True\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n",
    "# print(model)\n",
    "\n",
    "for i,param in enumerate(model.parameters()):\n",
    "    if i <35:\n",
    "        param.requires_grad=False\n",
    "    print(i,param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseActDense(\n",
      "    (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
      "    (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (act): ReLU()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print( model.encoder.block[0].layer[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"translation\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=4,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    "    push_to_hub=False,\n",
    "    logging_strategy='epoch'\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized__train,\n",
    "    eval_dataset=tokenized_eval,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"translationV2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = AutoModelForSeq2SeqLM.from_pretrained('translation\\checkpoint-25000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_trainer = Seq2SeqTrainer(\n",
    "    model=loaded_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized__train,\n",
    "    eval_dataset=tokenized_eval,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a7f8986765e4fd7ad0bf6a0f4daa7d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/188 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PredictionOutput(predictions=array([[    0,  4534,     3, ...,     0,     0,     0],\n",
       "       [    0,   316, 21583, ...,     1,     0,     0],\n",
       "       [    0,   316,  5877, ...,    67,    20, 19614],\n",
       "       ...,\n",
       "       [    0,   316,  2415, ...,    64,  1008,   402],\n",
       "       [    0,  1318, 13942, ...,  5007,     7,   173],\n",
       "       [    0,  3139,  9237, ...,  1199,    10,  2442]], dtype=int64), label_ids=array([[ 4534,     3,    15, ...,  -100,  -100,  -100],\n",
       "       [  644,     3, 20909, ...,  -100,  -100,  -100],\n",
       "       [  316,  5877,    29, ...,  -100,  -100,  -100],\n",
       "       ...,\n",
       "       [  316,  2974,  1847, ...,  -100,  -100,  -100],\n",
       "       [ 1318, 13942,    35, ...,  -100,  -100,  -100],\n",
       "       [ 3139,  9237,     3, ...,  -100,  -100,  -100]], dtype=int64), metrics={'test_loss': 1.1579248905181885, 'test_bleu1': 0.6142907406321468, 'test_bleu2': 0.36281222875880026, 'test_rougeL': 0.4404123001219971, 'test_runtime': 47.6737, 'test_samples_per_second': 62.907, 'test_steps_per_second': 3.943})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.predict(tokenized_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
